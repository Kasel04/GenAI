{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyUk7k6P6Zdvry84VnW0lW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kasel04/GenAI/blob/main/problem1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleans up text for less ram usage\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = text.replace(\"\\n\", \" \")  # Replace newlines with spaces\n",
        "    return text\n",
        "\n",
        "\n",
        "with open('combined_shakespeare.txt', 'r', encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "cleaned_text = clean_text(raw_text)\n",
        "print(f\"Cleaned text length: {len(cleaned_text)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZG7T3CJDPrL",
        "outputId": "f853f666-41f2-4ca4-b280-70a358b03cad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned text length: 238721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Tokenize the cleaned text\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "\n",
        "# Total number of unique words\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(f\"Total unique words: {total_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSgzHKQvFYuJ",
        "outputId": "7a6025cb-3969-4941-e406-dcc61f609642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 6482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input sequences\n",
        "input_sequences = []\n",
        "chunk_size = 500  # Limit the number of lines processed at a time to avoid memory overload\n",
        "\n",
        "# Process the text in smaller chunks\n",
        "for i in range(0, len(cleaned_text), chunk_size):\n",
        "    chunk = cleaned_text[i:i+chunk_size]\n",
        "    token_list = tokenizer.texts_to_sequences([chunk])[0]\n",
        "    for j in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:j+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "print(f\"Number of input sequences: {len(input_sequences)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmS4K9NLFbFX",
        "outputId": "e199fcfb-f795-4b53-88f7-9316754b6ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of input sequences: 43360\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Determine the maximum sequence length\n",
        "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
        "\n",
        "# Pad sequences\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Split sequences into X and y\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "\n",
        "y = np.eye(total_words)[y]\n",
        "\n",
        "print(\"Data preprocessing complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofeL2Jk2FcjE",
        "outputId": "86359b7e-3544-40bf-a05c-25840ecdf31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Define model parameters\n",
        "embedding_dim = 100\n",
        "lstm_units = 128  # Number of units in the LSTM layer\n",
        "vocab_size = total_words\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding layer to convert word indices to dense vectors\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_sequence_len-1))\n",
        "\n",
        "# LSTM layer\n",
        "model.add(LSTM(lstm_units, return_sequences=False))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "qyiS9EuqFpF4",
        "outputId": "7fc5fae9-9d2a-41f5-d471-ece6a4a547c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X, y, epochs=20, batch_size=64, verbose=1)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"text_generation_model.h5\")\n",
        "\n",
        "print(\"Model training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgz30RyuFtQY",
        "outputId": "3408fc65-e39b-4ccc-9ce9-bf8e8cb413a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 260ms/step - accuracy: 0.0239 - loss: 7.3023\n",
            "Epoch 2/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 263ms/step - accuracy: 0.0334 - loss: 6.6419\n",
            "Epoch 3/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 258ms/step - accuracy: 0.0444 - loss: 6.4209\n",
            "Epoch 4/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 259ms/step - accuracy: 0.0618 - loss: 6.1600\n",
            "Epoch 5/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 264ms/step - accuracy: 0.0732 - loss: 5.8707\n",
            "Epoch 6/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 259ms/step - accuracy: 0.0847 - loss: 5.6315\n",
            "Epoch 7/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 257ms/step - accuracy: 0.0922 - loss: 5.4051\n",
            "Epoch 8/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 257ms/step - accuracy: 0.1008 - loss: 5.1984\n",
            "Epoch 9/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 256ms/step - accuracy: 0.1180 - loss: 4.9673\n",
            "Epoch 10/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 256ms/step - accuracy: 0.1316 - loss: 4.7321\n",
            "Epoch 11/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 258ms/step - accuracy: 0.1531 - loss: 4.5307\n",
            "Epoch 12/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 257ms/step - accuracy: 0.1749 - loss: 4.3210\n",
            "Epoch 13/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 259ms/step - accuracy: 0.2069 - loss: 4.0948\n",
            "Epoch 14/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 264ms/step - accuracy: 0.2346 - loss: 3.9237\n",
            "Epoch 15/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 262ms/step - accuracy: 0.2645 - loss: 3.7215\n",
            "Epoch 16/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 258ms/step - accuracy: 0.3032 - loss: 3.5101\n",
            "Epoch 17/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 257ms/step - accuracy: 0.3288 - loss: 3.3570\n",
            "Epoch 18/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 257ms/step - accuracy: 0.3605 - loss: 3.1887\n",
            "Epoch 19/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 255ms/step - accuracy: 0.3913 - loss: 3.0285\n",
            "Epoch 20/20\n",
            "\u001b[1m678/678\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 258ms/step - accuracy: 0.4149 - loss: 2.8782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, next_words, model, tokenizer, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        # Tokenize and pad the seed text\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(token_list, verbose=0)\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Get the word corresponding to the predicted index\n",
        "        output_word = tokenizer.index_word[predicted_index]\n",
        "\n",
        "        # Append the predicted word to the seed text\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text\n",
        "\n",
        "# Generate text based on a seed prompt\n",
        "seed_prompt = \"To be, or not to be\"\n",
        "generated_text = generate_text(seed_prompt, 50, model, tokenizer, max_sequence_len)\n",
        "print(\"Generated text:\\n\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AAR4x1OWNwF",
        "outputId": "afdc0f7c-a046-447e-b071-f533a1b8997d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            " To be, or not to be and give thee from my love and in my verse when i have sworn thee fair and loving mourners be cxxxix o how to my love and you will be but for the time to my love and in my verse do i will dewe or since the world begun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing with more phrases\n",
        "test_phrases = [\n",
        "    \"All the world’s a stage\",\n",
        "    \"Shall I compare thee to a summer’s day\",\n",
        "    \"Once more unto the breach\"\n",
        "]\n",
        "\n",
        "for phrase in test_phrases:\n",
        "    generated = generate_text(phrase, 50, model, tokenizer, max_sequence_len)\n",
        "    print(f\"Seed phrase: {phrase}\")\n",
        "    print(f\"Generated text: {generated}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaQJYob9Wuoh",
        "outputId": "f6df8199-64ea-4906-86e9-976837004c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed phrase: All the world’s a stage\n",
            "Generated text: All the world’s a stage and in the first of beauty of a noted weed and beauty than a noted weed which for a perpetual dulness by the love have the love have no a vanish’d eyes then have i have seen and prove on the forests shook three summers’ class donatelink action of the\n",
            "--------------------------------------------------\n",
            "Seed phrase: Shall I compare thee to a summer’s day\n",
            "Generated text: Shall I compare thee to a summer’s day a very tall a day and vertuous i nur'st her old la you are you mer a very grosse man i am a very bitter sweeting it is a vertuous and vertuous i am a candle holder and soare with his gowne and his wife and a man that is\n",
            "--------------------------------------------------\n",
            "Seed phrase: Once more unto the breach\n",
            "Generated text: Once more unto the breach to be a candle holder and smilest vpon the stroke that murders me dead the churchyard came i will aduenture on your selfe and cut me here as i will not budge for a man to make me a ioyfull bride what a man to make me a ioyfull bride\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}